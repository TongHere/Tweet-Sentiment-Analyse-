{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03d328b-e192-4bf1-8b35-12ff64660dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sheet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import os\n",
    "from itertools import chain\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "#pip install spacy==2.2.3\n",
    "#python -m spacy download en_core_web_sm\n",
    "#pip install beautifulsoup4==4.9.1\n",
    "#pip install textblob==0.15.3\n",
    "from dataclasses import dataclass\n",
    "import spacy.cli\n",
    "import spacy\n",
    "#spacy.cli.download(\"en_core_web_sm\",'--no-deps')\n",
    "\n",
    "#download(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258be71c-d6e1-49d0-9115-37628b2a3253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-18 21:56:20.798000</td>\n",
       "      <td>@DSisourath The Thameslink core between London...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-13 07:31:53.122000</td>\n",
       "      <td>@DulwichHistory Loving the complaint about peo...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>delays</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-26 19:27:24.695000</td>\n",
       "      <td>@SW_Help .And yet you have no toilets on some ...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>toilets</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-26 19:28:49.281000</td>\n",
       "      <td>@SW_Help you have no toilets on some of your t...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>toilets</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-28 11:59:41.212000</td>\n",
       "      <td>@SpeedySticks007 @MrNeilJH @TLRailUK @christia...</td>\n",
       "      <td>-1.09125</td>\n",
       "      <td>50.79899</td>\n",
       "      <td>seats</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16944</th>\n",
       "      <td>2019-07-11 07:34:35</td>\n",
       "      <td>Haha oh man the audio corruption on @TLRailUK ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>announcements</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16945</th>\n",
       "      <td>2020-08-10 11:19:10.181000</td>\n",
       "      <td>@TLRailUK SweetIs there a plug to charge my ph...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>plugs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16946</th>\n",
       "      <td>2020-08-29 09:51:10.833000</td>\n",
       "      <td>@TLRailUK now there are far fewer commuters ha...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>tables</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16947</th>\n",
       "      <td>2020-11-02 12:06:06.967000</td>\n",
       "      <td>@geofftech I am voting for Thameslink. 1. in a...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>toilets</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16948</th>\n",
       "      <td>2020-11-02 12:06:06.967000</td>\n",
       "      <td>@geofftech I am voting for Thameslink. 1. in a...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>announcements</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16949 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                source_created_at  \\\n",
       "0      2020-09-18 21:56:20.798000   \n",
       "1      2020-10-13 07:31:53.122000   \n",
       "2      2020-10-26 19:27:24.695000   \n",
       "3      2020-10-26 19:28:49.281000   \n",
       "4      2020-09-28 11:59:41.212000   \n",
       "...                           ...   \n",
       "16944         2019-07-11 07:34:35   \n",
       "16945  2020-08-10 11:19:10.181000   \n",
       "16946  2020-08-29 09:51:10.833000   \n",
       "16947  2020-11-02 12:06:06.967000   \n",
       "16948  2020-11-02 12:06:06.967000   \n",
       "\n",
       "                                                    text  longitude  latitude  \\\n",
       "0      @DSisourath The Thameslink core between London...   -0.12574  51.50853   \n",
       "1      @DulwichHistory Loving the complaint about peo...   -0.12574  51.50853   \n",
       "2      @SW_Help .And yet you have no toilets on some ...   -0.12574  51.50853   \n",
       "3      @SW_Help you have no toilets on some of your t...   -0.12574  51.50853   \n",
       "4      @SpeedySticks007 @MrNeilJH @TLRailUK @christia...   -1.09125  50.79899   \n",
       "...                                                  ...        ...       ...   \n",
       "16944  Haha oh man the audio corruption on @TLRailUK ...        NaN       NaN   \n",
       "16945  @TLRailUK SweetIs there a plug to charge my ph...   -0.12574  51.50853   \n",
       "16946  @TLRailUK now there are far fewer commuters ha...   -0.12574  51.50853   \n",
       "16947  @geofftech I am voting for Thameslink. 1. in a...   -0.12574  51.50853   \n",
       "16948  @geofftech I am voting for Thameslink. 1. in a...   -0.12574  51.50853   \n",
       "\n",
       "               topic sentiment  \n",
       "0            service  negative  \n",
       "1             delays  negative  \n",
       "2            toilets  negative  \n",
       "3            toilets  negative  \n",
       "4              seats   neutral  \n",
       "...              ...       ...  \n",
       "16944  announcements   neutral  \n",
       "16945          plugs   neutral  \n",
       "16946         tables   neutral  \n",
       "16947        toilets   neutral  \n",
       "16948  announcements   neutral  \n",
       "\n",
       "[16949 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\sheet\\Dataset\\Tweets\\tweets_ws22_v1.json\"\n",
    "\n",
    "with open(path) as f:\n",
    "   \n",
    "    dump = json.loads(f.read())\n",
    "    org_df = pd.json_normalize(dump)\n",
    "    org_df\n",
    "    \n",
    "final_df = pd.DataFrame() \n",
    "\n",
    "for i in range(0,len(dump)):\n",
    "    temp_topic_df = pd.json_normalize(dump[i]['labels']['topic'])\n",
    "    temp_sentiment_df = pd.json_normalize(dump[i]['labels']['sentiment'])\n",
    "    temp_topic_df.rename(columns={'id':'topic_id','tweet_id':'topic_tweet_id','user_id':'topic_user_id','ground_truth':'topic_ground_truth'}, inplace = True)\n",
    "    temp_sentiment_df.rename(columns={'id':'sentiment_id','tweet_id':'sentiment_tweet_id','user_id':'sentiment_user_id','ground_truth':'sentiment_ground_truth'}, inplace = True)\n",
    "    df_join_topic_sent = temp_topic_df.join(temp_sentiment_df)\n",
    "    final_df = final_df.append(df_join_topic_sent)\n",
    "    #final_df = pd.concat([org_df,Joined_df],axis = 1)\n",
    "    \n",
    "    \n",
    "cols = final_df.columns\n",
    "org_df[cols] = final_df[cols].values\n",
    "\n",
    "\n",
    "data =  org_df.drop(columns=['labels.topic','labels.sentiment','topic_tweet_id','sentiment_tweet_id','author_id','source','topic_user_id','topic_ground_truth','language','id','source_id','relevant','sentiment_ground_truth','sentiment_id','topic_id','sentiment_user_id'])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4445d238-1086-455c-bc67-cd97ddc31558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16949 entries, 0 to 16948\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   source_created_at  16949 non-null  object \n",
      " 1   text               16949 non-null  object \n",
      " 2   longitude          1425 non-null   float64\n",
      " 3   latitude           1425 non-null   float64\n",
      " 4   topic              16949 non-null  object \n",
      " 5   sentiment          16949 non-null  object \n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 794.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a521f3e6-1ba6-453a-9dbc-4ee9fb12fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install text-hammer==0.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5177c9a8-d359-4514-977c-7bf4aadbcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_hammer as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866f2043-81de-46df-baaa-5e305466d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "def text_preprocessing(df,col_name):\n",
    "    column = col_name\n",
    "    df[column] = df[column].progress_apply(lambda x:str(x).lower())\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n",
    "#     df[column] = df[column].progress_apply(lambda x: ps.remove_stopwords(x))\n",
    "\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n",
    "    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe14074-1cef-42e5-9b4c-f694f496d98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872879b71eed4a2caa1277a498d5bb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4947c5b5424992ae583d5132464dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b0d9bc374d453993ed81a6ca63e099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheet\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e344ea4eb54420a138c7fa50cbfbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf33d5a244c4866922011cc75f5f906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-18 21:56:20.798000</td>\n",
       "      <td>dsisourath the thameslink core between london ...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-13 07:31:53.122000</td>\n",
       "      <td>dulwichhistory loving the complaint about peop...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>delays</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-26 19:27:24.695000</td>\n",
       "      <td>sw_help and yet you have no toilets on some of...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>toilets</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-26 19:28:49.281000</td>\n",
       "      <td>sw_help you have no toilets on some of your tr...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>toilets</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-28 11:59:41.212000</td>\n",
       "      <td>speedysticks007 mrneiljh tlrailuk christianwol...</td>\n",
       "      <td>-1.09125</td>\n",
       "      <td>50.79899</td>\n",
       "      <td>seats</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source_created_at  \\\n",
       "0  2020-09-18 21:56:20.798000   \n",
       "1  2020-10-13 07:31:53.122000   \n",
       "2  2020-10-26 19:27:24.695000   \n",
       "3  2020-10-26 19:28:49.281000   \n",
       "4  2020-09-28 11:59:41.212000   \n",
       "\n",
       "                                                text  longitude  latitude  \\\n",
       "0  dsisourath the thameslink core between london ...   -0.12574  51.50853   \n",
       "1  dulwichhistory loving the complaint about peop...   -0.12574  51.50853   \n",
       "2  sw_help and yet you have no toilets on some of...   -0.12574  51.50853   \n",
       "3  sw_help you have no toilets on some of your tr...   -0.12574  51.50853   \n",
       "4  speedysticks007 mrneiljh tlrailuk christianwol...   -1.09125  50.79899   \n",
       "\n",
       "     topic sentiment  \n",
       "0  service  negative  \n",
       "1   delays  negative  \n",
       "2  toilets  negative  \n",
       "3  toilets  negative  \n",
       "4    seats   neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = text_preprocessing(data,'text')\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221e262d-0deb-4b31-81e1-072d0ae9d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for Bert Architecture format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33530023-18bc-4a66-85b6-e5fee6158be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['num_words'] = cleaned_df.text.apply(lambda x:len(x.split()))\n",
    "cleaned_df.num_words.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fef7ea-de6b-4c69-834d-ffff700c2997",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        negative\n",
       "1        negative\n",
       "2        negative\n",
       "3        negative\n",
       "4         neutral\n",
       "           ...   \n",
       "16944     neutral\n",
       "16945     neutral\n",
       "16946     neutral\n",
       "16947     neutral\n",
       "16948     neutral\n",
       "Name: sentiment, Length: 16949, dtype: category\n",
       "Categories (3, object): ['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the data type to the category to encode into codes \n",
    "cleaned_df['sentiment'] = cleaned_df.sentiment.astype('category')\n",
    "cleaned_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15072fb-dcc9-4afa-b53b-99873cf62b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "16944    1\n",
       "16945    1\n",
       "16946    1\n",
       "16947    1\n",
       "16948    1\n",
       "Name: sentiment, Length: 16949, dtype: int8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['sentiment']  =  cleaned_df.sentiment.cat.codes\n",
    "cleaned_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23fd47ec-d129-490f-a79a-ac9777096961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-18 21:56:20.798000</td>\n",
       "      <td>dsisourath the thameslink core between london ...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>service</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-13 07:31:53.122000</td>\n",
       "      <td>dulwichhistory loving the complaint about peop...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>delays</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-26 19:27:24.695000</td>\n",
       "      <td>sw_help and yet you have no toilets on some of...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>toilets</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-26 19:28:49.281000</td>\n",
       "      <td>sw_help you have no toilets on some of your tr...</td>\n",
       "      <td>-0.12574</td>\n",
       "      <td>51.50853</td>\n",
       "      <td>toilets</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-28 11:59:41.212000</td>\n",
       "      <td>speedysticks007 mrneiljh tlrailuk christianwol...</td>\n",
       "      <td>-1.09125</td>\n",
       "      <td>50.79899</td>\n",
       "      <td>seats</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source_created_at  \\\n",
       "0  2020-09-18 21:56:20.798000   \n",
       "1  2020-10-13 07:31:53.122000   \n",
       "2  2020-10-26 19:27:24.695000   \n",
       "3  2020-10-26 19:28:49.281000   \n",
       "4  2020-09-28 11:59:41.212000   \n",
       "\n",
       "                                                text  longitude  latitude  \\\n",
       "0  dsisourath the thameslink core between london ...   -0.12574  51.50853   \n",
       "1  dulwichhistory loving the complaint about peop...   -0.12574  51.50853   \n",
       "2  sw_help and yet you have no toilets on some of...   -0.12574  51.50853   \n",
       "3  sw_help you have no toilets on some of your tr...   -0.12574  51.50853   \n",
       "4  speedysticks007 mrneiljh tlrailuk christianwol...   -1.09125  50.79899   \n",
       "\n",
       "     topic  sentiment  num_words  \n",
       "0  service          0         21  \n",
       "1   delays          0         19  \n",
       "2  toilets          0         27  \n",
       "3  toilets          0         29  \n",
       "4    seats          1         37  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95efa6e-5b8b-48a0-9594-c6c574d144cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "to_categorical(cleaned_df.sentiment,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e379cf-dc59-4549-bd45-80581d1fa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train,data_test = train_test_split(cleaned_df, test_size = 0.3, random_state = 42, stratify = cleaned_df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd888801-1ebf-4954-8825-7b091b299276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11864, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf8ef95-bf2a-4773-a40b-d8edd53af9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5085, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bde7f5c0-06f4-4205-ac09-3d215b69198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7440\n",
       "1    4255\n",
       "2     169\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d019bbc-7c30-46fd-9c51-6d7964052186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import AutoTokenizer,TFBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2290488f-cd1e-424f-96fb-7370e88a8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input  \n",
    "\n",
    "x_train = tokenizer(\n",
    "    text=data_train.text.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=70,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "\n",
    "x_test = tokenizer(\n",
    "    text=data_test.text.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=70,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7154aea-9747-4c9a-99d1-f247089965a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheet\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1 2], y=11829    0\n",
      "611      0\n",
      "10549    0\n",
      "12659    1\n",
      "6450     0\n",
      "        ..\n",
      "685      0\n",
      "15314    0\n",
      "4881     0\n",
      "5541     0\n",
      "14699    0\n",
      "Name: sentiment, Length: 11864, dtype: int8 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "#Calculate weights \n",
    "weights = class_weight.compute_class_weight('balanced',np.unique(data_train['sentiment']),data_train['sentiment'])\n",
    "weights\n",
    "\n",
    "#transform array to dictionary\n",
    "weights = dict(enumerate(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8659df-22a7-4e6b-b663-d6ebe69531e4",
   "metadata": {},
   "source": [
    "cleaned_df['sentiment']  =  cleaned_df.sentiment.cat.codes\n",
    "cleaned_df.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b17f8-7927-4a96-8b7b-b999318e7a0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e5882fb-f1fd-45c2-bac2-2a65a30bdffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75fffdda-6c9c-471d-9de3-06f4ec254156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd28ccff-2d01-4370-b701-e07ad0d85e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "   \n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "\n",
    "\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "\n",
    "y = Dense(3,activation = 'sigmoid')(out)\n",
    "    \n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "#return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "627da7ac-e8b3-40ba-aafa-5b19deaea14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 70)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 70)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 70,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          98432       ['global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)           (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           4128        ['dropout_75[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3)            99          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,412,931\n",
      "Trainable params: 108,412,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f17d8769-e473-488c-b2a4-5356fd618da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "197c58e2-2958-44e1-86e0-41a8e4c613cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate = 5e-05, epsilon=1e-08, decay =0.01,clipnorm= 1.0)\n",
    "\n",
    "loss = CategoricalCrossentropy(from_logits = True)\n",
    "metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer =optimizer,loss =loss,metrics =metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "446aaa51-49f4-400e-87d0-9ccf4f3fcfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe287269-96d6-4d54-b7aa-93cfacc17bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fit and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc57d62-899f-4a47-b0df-abb4f463bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheet\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sheet\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  1/170 [..............................] - ETA: 7:43:30 - loss: 1.4793 - balanced_accuracy: 0.2857WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  2/170 [..............................] - ETA: 6:02:31 - loss: 1.1685 - balanced_accuracy: 0.4643WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  3/170 [..............................] - ETA: 5:30:03 - loss: 1.0477 - balanced_accuracy: 0.5095WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  4/170 [..............................] - ETA: 5:26:49 - loss: 0.9612 - balanced_accuracy: 0.5214WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  5/170 [..............................] - ETA: 5:17:41 - loss: 0.9438 - balanced_accuracy: 0.5200WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  6/170 [>.............................] - ETA: 5:19:30 - loss: 0.9163 - balanced_accuracy: 0.5190WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  7/170 [>.............................] - ETA: 5:13:21 - loss: 0.8875 - balanced_accuracy: 0.5184WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  8/170 [>.............................] - ETA: 5:10:59 - loss: 0.8651 - balanced_accuracy: 0.5357WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "  9/170 [>.............................] - ETA: 5:13:07 - loss: 0.8532 - balanced_accuracy: 0.5476WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 10/170 [>.............................] - ETA: 5:10:57 - loss: 0.8378 - balanced_accuracy: 0.5571WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 11/170 [>.............................] - ETA: 5:04:28 - loss: 0.8114 - balanced_accuracy: 0.5714WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 12/170 [=>............................] - ETA: 4:59:14 - loss: 0.7955 - balanced_accuracy: 0.5762WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 13/170 [=>............................] - ETA: 4:56:20 - loss: 0.7920 - balanced_accuracy: 0.5736WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 14/170 [=>............................] - ETA: 4:56:25 - loss: 0.7846 - balanced_accuracy: 0.5806WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 15/170 [=>............................] - ETA: 4:56:22 - loss: 0.7843 - balanced_accuracy: 0.5829WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 16/170 [=>............................] - ETA: 4:55:21 - loss: 0.7794 - balanced_accuracy: 0.5848WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 17/170 [==>...........................] - ETA: 4:52:56 - loss: 0.7713 - balanced_accuracy: 0.5899WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 18/170 [==>...........................] - ETA: 4:46:45 - loss: 0.7668 - balanced_accuracy: 0.5913WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 19/170 [==>...........................] - ETA: 4:42:26 - loss: 0.7614 - balanced_accuracy: 0.6000WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 20/170 [==>...........................] - ETA: 4:40:21 - loss: 0.7639 - balanced_accuracy: 0.6036WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 21/170 [==>...........................] - ETA: 4:36:47 - loss: 0.7619 - balanced_accuracy: 0.6068WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 22/170 [==>...........................] - ETA: 4:31:56 - loss: 0.7596 - balanced_accuracy: 0.6071WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 23/170 [===>..........................] - ETA: 4:32:14 - loss: 0.7531 - balanced_accuracy: 0.6124WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 24/170 [===>..........................] - ETA: 4:26:53 - loss: 0.7567 - balanced_accuracy: 0.6107WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 25/170 [===>..........................] - ETA: 4:27:25 - loss: 0.7527 - balanced_accuracy: 0.6131WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 26/170 [===>..........................] - ETA: 4:27:34 - loss: 0.7533 - balanced_accuracy: 0.6148WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 27/170 [===>..........................] - ETA: 4:28:03 - loss: 0.7534 - balanced_accuracy: 0.6122WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 28/170 [===>..........................] - ETA: 4:26:50 - loss: 0.7478 - balanced_accuracy: 0.6163WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 29/170 [====>.........................] - ETA: 4:24:00 - loss: 0.7495 - balanced_accuracy: 0.6192WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 30/170 [====>.........................] - ETA: 4:23:09 - loss: 0.7440 - balanced_accuracy: 0.6224WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 31/170 [====>.........................] - ETA: 4:22:23 - loss: 0.7410 - balanced_accuracy: 0.6230WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 32/170 [====>.........................] - ETA: 4:18:21 - loss: 0.7397 - balanced_accuracy: 0.6223WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 33/170 [====>.........................] - ETA: 4:14:50 - loss: 0.7375 - balanced_accuracy: 0.6229WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 34/170 [=====>........................] - ETA: 4:12:45 - loss: 0.7354 - balanced_accuracy: 0.6239WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 35/170 [=====>........................] - ETA: 4:10:39 - loss: 0.7308 - balanced_accuracy: 0.6257WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 36/170 [=====>........................] - ETA: 4:09:09 - loss: 0.7304 - balanced_accuracy: 0.6246"
     ]
    }
   ],
   "source": [
    "train = model.fit(x= {'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']},\n",
    "                  y=to_categorical(data_train.sentiment,3),\n",
    "                  validation_data= ({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']},to_categorical(data_test.sentiment,3)),\n",
    "                  epochs =1,\n",
    "                  batch_size =70\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6aad7-3a64-457b-97b4-b6d9cd9e4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model prediction without balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64609a-c860-4a22-9aca-48b45d4f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fd674-e9ab-4c1a-a36a-18bf3a9bf484",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.argmax(predict,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ddc28-75dc-4ea4-82d0-8c6e4ed9f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.sentiment #ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82269c-d9ed-48f5-8aa5-e2ce2e94063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(data_test.sentiment,y_predicted ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca0a74-401f-4459-8c7b-db62b6ffd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700e6a7-9c07-4ba3-9450-ea7d7fd1b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = input(str('input the text'))\n",
    "\n",
    "x_val = tokenizer(\n",
    "        text=input_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=70,truncation=True,\n",
    "        padding='max_length', \n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,verbose = True)\n",
    "validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100 \n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d0edd-7c08-4ff9-803c-ff2bac585eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict  = {'negative':0,'neutral':1, 'positive':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e660683-8b77-4fa1-984f-de9aaee65584",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in zip(encoded_dict.keys(),validation[0]):\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960eb3ec-606c-4a9d-8913-66facb9260b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  after balancing using class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264abe22-d604-40f1-a073-71f688fa0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model.fit (x= {'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']},\n",
    "                  y=to_categorical(data_train.sentiment,3),\n",
    "                  validation_data= ({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']},to_categorical(data_test.sentiment,3)),\n",
    "                  epochs =1,\n",
    "                  batch_size =36,\n",
    "                  class_weight =weights\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0299c-a590-496c-b0a5-d662806e0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380705f0-4c8a-4587-a0de-b5f1d251b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.argmax(predicted,axis =1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
